{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c0c5b6-0435-44d9-aedf-85adf923eb59",
   "metadata": {},
   "source": [
    "<img src=\"imgs/hpe_logo.png\" alt=\"HPE Logo\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c37a8-f4b6-45a3-ac61-cc8a0ac2962d",
   "metadata": {},
   "source": [
    "# Chicago DS Summit Workshop: Chatbot tutorial: Finetuning GPT models at Scale with Machine Learning Development Environment\n",
    " ----\n",
    "\n",
    "Note this Demo is based on https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "\n",
    "## Objective: Train your own chatbot\n",
    "This notebook walks you through finetuning your own chatbot.\n",
    "We will learn what are Generative Pretrained Transformers (GPT), how to finetune to your own domain, and finetune at Scale\n",
    "\n",
    "## Why is this exciting: The rise of generative language modeling\n",
    "Generative Language models like GPT-4 and ChatGPT enable exciting applications that were not possible before!\n",
    "\n",
    "* `Enterprise`: Chatbots for helpdesk support\n",
    "* `Healthcare`: Chatbots for scheduling appts, manage coverage, process claims\n",
    "* `Manufacturing`: Chatbots for checking supplies and inventory check\n",
    "* `Financial Services`: Chatbots for investment and account support\n",
    "\n",
    "With Machine Learning Development Environment (MLDE, based on DeterminedAI) we help engineers create powerful language modeling application at scale!\n",
    "\n",
    "## Why MLDE and DeterminedAI\n",
    "\n",
    "Developing robust, high performing Deep Learning application is challenging. Succesful team requires data, compute, and great infrastructure. Building and managing distributed training, automatic checkpointing, hyperparameter search and metrics tracking is critical. \n",
    "\n",
    "MLDE can remove the burden of writing and maintaining a custom training harness and offers a streamlined approach to onboard new models to a state-of-the-art training platform, offering the following integrated platform features:\n",
    "\n",
    "<img src=\"imgs/det_components.jpg\" alt=\"Determined Components\" width=\"900\">\n",
    "\n",
    "Determined provides a high-level framework APIs for PyTorch, Keras, and Estimators that let users describe their model without boilerplate code. Determined reduces boilerplate by providing a state-of-the-art training loop that provides distributed training, hyperparameter search, automatic mixed precision, reproducibility, and many more features.\n",
    "\n",
    "<h3>Overview of this workshop</h3>\n",
    "\n",
    "## Overview of Tutorial\n",
    "\n",
    "* Intro to GPT models\n",
    "* Data Science Chatbot Baseline: Chatbot to ask Data Science questions\n",
    "* Finetune GPT model on a Data Science book to better answer Data Science Tasks\n",
    "* Overview of integrating Pytorch training code into MLDE\n",
    "* English to Latex Chatbot: Convert english to latex\n",
    "* Finetune GPT model on dataset to convert english to latex\n",
    "* User exercise: Finetune bot on custom text data\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c271e3-0744-4c54-a4b9-dcd5dd9a4ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, GPT2LMHeadModel, pipeline, \\\n",
    "                         Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c557e7d-3d7c-4629-9b0a-efa9b1effbd5",
   "metadata": {},
   "source": [
    "# What is GPT\n",
    "\n",
    "<img src=\"imgs/openAI-gpt2.png\" alt=\"Determined Components\" width=\"900\">\n",
    "\n",
    "GPT2 is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.\n",
    "\n",
    "Other models available: GPT2-Medium, GPT2-Large and GPT2-XL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a31e5b-8147-424f-8a5c-0c1367f47c6d",
   "metadata": {},
   "source": [
    "# Data Science Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b7537-1345-43d3-bef9-420a6b9266c5",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "We will load a pretrained model on X dataset and see how it responds to data science questions.\n",
    "\n",
    "Prompt: `A test statistic is a value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bdd68c9-e820-4872-8577-3a3ec23024d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# load up a GPT2 model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pretrained_generator \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m}\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # load up a GPT2 model\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', model=model, tokenizer='gpt2',\n",
    "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d3cbc58-8870-4bf3-a39a-d86b868cab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='A test statistic is a value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f0f56d2-6208-4a7e-ab34-c16b41e2698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "A test statistic is a value. A value gives a statistic. A statistic is a numerical statistic for the last time you evaluated it. An exponent makes a value equal to the number of digits that you can use to represent the number of digits you need\n",
      "----------\n",
      "A test statistic is a value between 0 and 8 indicating that the number of people using the service will exceed the number of users the service allows; see the test statistic for more.\n",
      "\n",
      "Table 15 has the following number of applications run, as of\n",
      "----------\n",
      "A test statistic is a value generated as a function that expresses the sum of the sum of the components required to be reached. For instance, \"y\" and it will both return 0.5 or 1 in C:\n",
      "\n",
      "[a.i\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('----------')\n",
    "for generated_sequence in pretrained_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86215-287d-47b8-8f16-d9942fd919cc",
   "metadata": {},
   "source": [
    "## Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833d407a-0cd3-44b3-a909-b44f00d7ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing files to send to master... 19.8KB and 11 files\n",
      "Created experiment 1074\n"
     ]
    }
   ],
   "source": [
    "!det experiment create \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/const_ds_chatbot.yaml \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32982e-834b-404d-ba3e-e6adc3bff3e5",
   "metadata": {},
   "source": [
    "## See Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c54ab72-b109-4b9b-a877-ab6d4ca82435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Local checkpoint path:\n",
      "checkpoints/38443886-003c-4f28-97ae-108992665643 \n",
      "\n",
      "   Experiment ID |   Trial ID |   Steps Completed | Report Time              | Checkpoint UUID                      | Validation Metrics                       | Metadata\n",
      "-----------------+------------+-------------------+--------------------------+--------------------------------------+------------------------------------------+----------------------------------------\n",
      "              33 |         33 |               137 | 2023-03-24 03:00:52+0000 | 38443886-003c-4f28-97ae-108992665643 | {                                        | {\n",
      "                 |            |                   |                          |                                      |     \"avgMetrics\": {                      |     \"determined_version\": \"0.20.0\",\n",
      "                 |            |                   |                          |                                      |         \"eval_loss\": 3.0931387434200364, |     \"format\": \"pickle\",\n",
      "                 |            |                   |                          |                                      |         \"perplexity\": 22.546680450439453 |     \"framework\": \"torch-1.10.2+cu113\",\n",
      "                 |            |                   |                          |                                      |     },                                   |     \"steps_completed\": 137\n",
      "                 |            |                   |                          |                                      |     \"batchMetrics\": []                   | }\n",
      "                 |            |                   |                          |                                      | }                                        |\n"
     ]
    }
   ],
   "source": [
    "!det checkpoint download 38443886-003c-4f28-97ae-108992665643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e38e8fe0-9030-4d3a-bfdb-5b509450a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ./checkpoints/71dda373-9065-4536-81e0-4acceb7db318/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e2582a-50fd-43f6-a1d3-efd7fcfced6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "ckpt = torch.load('./checkpoints/38443886-003c-4f28-97ae-108992665643/state_dict.pth')\n",
    "# print(len(ckpt['models_state_dict']))\n",
    "loaded_model.load_state_dict(ckpt['models_state_dict'][0])\n",
    "\n",
    "finetuned_generator = pipeline(\n",
    "    'text-generation', model=loaded_model, tokenizer=tokenizer,\n",
    "    config={'max_length': 200,  'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dd448dc-f45a-4d29-8405-93c75721e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='A test statistic is a value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "624d94e5-11f1-4bd7-9518-c59eeac7c1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "A test statistic is a value\n",
      "that has no standard deviation. This means that the actual value of a t-test is\n",
      "much, much lower. Once again, our model will always be able to find an exact value.\n",
      "We can easily\n",
      "----------\n",
      "A test statistic is a value that is expressed as a number that is associated with probability. This method tells us how much better\n",
      "we can perform this statistic using statistics. To know this, let's calculate the number of tests performed and then make our\n",
      "----------\n",
      "A test statistic is a value for the measure of the expected value of the variable (which means that if both we (i.e. everyone) are\n",
      "focusing on the same variables, we will be doing a slightly different set of tasks).\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('----------')\n",
    "for generated_sequence in finetuned_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2484ebd-c5dd-41c8-b80c-56bc7e5a48fb",
   "metadata": {},
   "source": [
    "# Overview of integrating Pytorch training code into MLDE\n",
    "\n",
    "The main components for any deep learning training loop are the following:\n",
    "* Datasets\n",
    "* Dataloader\n",
    "* Model\n",
    "* Optimizer\n",
    "* (Optional) Learn rate schedule\n",
    "* training a batch, evaluating a batch\n",
    "\n",
    "We will show how to integrate each core part into MLDE using the PyTorchTrial API. Note we have another API called CoreAPI, that supports flexibility if your team wants to integrate more complex Machine Learning codebases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951811f1-d9bf-4750-8599-4e9089919121",
   "metadata": {},
   "source": [
    "### Template Class that integrates DL code with MLDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afbaa0-0c2f-48be-a702-aef394a7670a",
   "metadata": {},
   "source": [
    "```python\n",
    "import filelock\n",
    "import os\n",
    "from typing import Any, Dict, Sequence, Tuple, Union, cast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from determined.pytorch import DataLoader, PyTorchTrial, PyTorchTrialContext\n",
    "\n",
    "import data\n",
    "\n",
    "TorchData = Union[Dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor]\n",
    "\n",
    "class GPT2Trial(PyTorchTrial):\n",
    "    def __init__(self, context: PyTorchTrialContext) -> None:\n",
    "        # Trial context contains info about the trial, such as the hyperparameters for training\n",
    "        self.context = context\n",
    "        \n",
    "        # init and wrap model, optimizer, LRscheduler, datasets\n",
    "       \n",
    "\n",
    "    def build_training_data_loader(self) -> DataLoader:\n",
    "        # create train dataloader from dataset\n",
    "        return DataLoader()\n",
    "\n",
    "    def build_validation_data_loader(self) -> DataLoader:\n",
    "        # create train dataloader from dataset\n",
    "        return DataLoader()\n",
    "\n",
    "    def train_batch(self, batch: TorchData, epoch_idx: int, batch_idx: int)  -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def evaluate_batch(self, batch: TorchData) -> Dict[str, Any]:\n",
    "        return {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182d059-603a-40ef-9d04-eab7d70eb884",
   "metadata": {},
   "source": [
    "### Wrap Model\n",
    "* Wrapping model to the TrialContext allows MLDE to reduces boilerplate code\n",
    "* Providing a state-of-the-art training loop that provides distributed training, hyperparameter search, automatic mixed precision, reproducibility, and many more features\n",
    "* All the models, optimizers, and LR schedulers must be wrapped with wrap_model and wrap_optimizer respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7c6b2-1b6a-4347-a552-cfeb42169bd0",
   "metadata": {},
   "source": [
    "```python\n",
    "self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Wrapping model to the TrialContext \n",
    "self.model = self.context.wrap_model(self.model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1477398-8e18-4056-b1ce-398e48cd1d51",
   "metadata": {},
   "source": [
    "### Wrap Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4051af-bc83-43bb-acb0-b5529d402fe8",
   "metadata": {},
   "source": [
    "```python\n",
    "self.optimizer = self.context.wrap_optimizer(\n",
    "            AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.adam_epsilon)\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92578625-94e2-47eb-8304-aee0a00be52c",
   "metadata": {},
   "source": [
    "### Wrap LR scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a5c51-d70c-46be-b976-4e5946c2c021",
   "metadata": {},
   "source": [
    "```python\n",
    "self.scheduler = self.context.wrap_lr_scheduler(\n",
    "    get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=self.warmup_steps,\n",
    "                                    num_training_steps=self.t_total),\n",
    "    LRScheduler.StepMode.MANUAL_STEP\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd085b7-6a66-4aeb-b1b6-cba5cc3bb126",
   "metadata": {},
   "source": [
    "### Integrate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf815c-2c3e-44a4-bc1a-dfb84eee703b",
   "metadata": {},
   "source": [
    "```python\n",
    "dataset = TextDataset(\n",
    "                tokenizer=tokenizer,\n",
    "                file_path='/run/determined/workdir/shared_fs/workshop_data/hamlet.txt',\n",
    "                block_size=32  # length of each chunk of text to use as a datapoint\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed797aff-ce43-4492-8810-a1295a2c9f60",
   "metadata": {},
   "source": [
    "### Implement Train Dataloader and Validation Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0b04b-b4ab-4eb0-85e0-9f302b2149ae",
   "metadata": {},
   "source": [
    "```python\n",
    "def build_training_data_loader(self) -> None:\n",
    "    '''\n",
    "    '''\n",
    "    self.train_sampler = RandomSampler(self.dataset)\n",
    "    self.train_dataloader = DataLoader(self.dataset, collate_fn =self.data_collator ,sampler=self.train_sampler, batch_size=self.train_batch_size)\n",
    "    return self.train_dataloader\n",
    "def build_validation_data_loader(self) -> None:\n",
    "    '''\n",
    "    '''\n",
    "    self.eval_sampler = SequentialSampler(self.dataset)\n",
    "    self.validataion_dataloader = DataLoader(self.dataset,collate_fn =self.data_collator, sampler=self.eval_sampler, batch_size=self.eval_batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20951d8-a14f-45cc-81e2-2b39b0b3f608",
   "metadata": {},
   "source": [
    "### Implement Train Batch and Evaluate Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ed9c9-7be5-47a8-a8c4-5f3c0b30e869",
   "metadata": {},
   "source": [
    "```python\n",
    "def train_batch(self,batch,epoch_idx, batch_idx):\n",
    "    '''\n",
    "    '''\n",
    "    inputs,labels = self.format_batch(batch)\n",
    "    outputs = self.model(inputs, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    train_result = {\n",
    "        'loss': loss\n",
    "    }\n",
    "    self.context.backward(train_result[\"loss\"])\n",
    "    self.context.step_optimizer(self.optimizer)\n",
    "    return train_result\n",
    "\n",
    "def evaluate_batch(self,batch):\n",
    "    '''\n",
    "    '''\n",
    "    inputs,labels = self.format_batch(batch)\n",
    "    outputs = self.model(inputs, labels=labels)\n",
    "    lm_loss = outputs[0]\n",
    "    eval_loss = lm_loss.mean().item()\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    results = {\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e528c-a9db-4f28-a4d5-37115a39860d",
   "metadata": {},
   "source": [
    "# Walkthrough of Finetuning Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a53637-deaf-4ea6-a304-7d51a9db2522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97bb77c8-6010-43ff-9bb6-ef49ef61ff3e",
   "metadata": {},
   "source": [
    "# English 2 Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be3455-aec5-43de-ba08-186e3c1386ca",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce5f14a9-2ce1-4453-94a4-fd9a5f3bb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that a non-finetuned model could not have done this\n",
    "MODEL='gpt2'\n",
    "non_finetuned_latex_generator = pipeline(\n",
    "    'text-generation', \n",
    "    model=GPT2LMHeadModel.from_pretrained(MODEL),  # not fine-tuned!\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f3cb0b6-16f5-431a-985a-93c4a85bf94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX:\n",
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f of x is\n"
     ]
    }
   ],
   "source": [
    "# Add our singular prompt\n",
    "CONVERSION_PROMPT = 'LCT\\n'  # LaTeX conversion task\n",
    "\n",
    "CONVERSION_TOKEN = 'LaTeX:'\n",
    "text_sample = 'f of x is sum from 0 to x of x squared'\n",
    "# text_sample = 'f of x equals x squared'\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(conversion_text_sample)\n",
    "print(non_finetuned_latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730dfb3-84ce-4160-8adb-bd165aba76f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb7d1341-94ee-4fb9-a85e-8ab6dfb8075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(non_finetuned_latex_generator(\n",
    "#     conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "#     max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    "# )[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42ad3a-ad9f-4f4b-8f55-0530685757bd",
   "metadata": {},
   "source": [
    "## Dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3797380-ca2a-42fc-b718-3e20532faa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>LaTeX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>integral from a to b of x squared</td>\n",
       "      <td>\\int_{a}^{b} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>integral from negative 1 to 1 of x squared</td>\n",
       "      <td>\\int_{-1}^{1} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      English                   LaTeX\n",
       "0           integral from a to b of x squared   \\int_{a}^{b} x^2 \\,dx\n",
       "1  integral from negative 1 to 1 of x squared  \\int_{-1}^{1} x^2 \\,dx"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/english_to_latex.csv')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "209fa6f8-0b8f-436b-9dac-56ec209a0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8368f2a-4f52-4f55-840c-50edae606dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our singular prompt\n",
    "CONVERSION_PROMPT = 'LCT\\n'  # LaTeX conversion task\n",
    "\n",
    "CONVERSION_TOKEN = 'LaTeX:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49ccee9e-cc98-46ae-8f07-ac9e1a8e4b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: integral from a to b of x squared\n",
      "LaTeX: \\int_{a}^{b} x^2 \\,dx\n"
     ]
    }
   ],
   "source": [
    "# This is our \"training prompt\" that we want GPT2 to recognize and learn\n",
    "training_examples = f'{CONVERSION_PROMPT}English: ' + data['English'] + '\\n' + CONVERSION_TOKEN + ' ' + data['LaTeX'].astype(str)\n",
    "\n",
    "print(training_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c04c8483-2824-4449-b4fe-090369bfa462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LCT\\nEnglish: integral from a to b of x square...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LCT\\nEnglish: integral from negative 1 to 1 of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  LCT\\nEnglish: integral from a to b of x square...\n",
       "1  LCT\\nEnglish: integral from negative 1 to 1 of..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df = pd.DataFrame({'text': training_examples})\n",
    "\n",
    "task_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3a003-8be6-47e4-b924-8727725c90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_data = Dataset.from_pandas(task_df)  # turn a pandas DataFrame into a Dataset\n",
    "\n",
    "def preprocess(examples):  # tokenize our text but don't pad because our collator will pad for us dynamically\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "latex_data = latex_data.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb779568-71ab-42f0-bb62-b9edf4e0fc8b",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5f5f7-9d4a-4da5-9ad9-a31d5b216811",
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/const_eng_to_latex.yaml \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c70a3d-1660-4a5c-9beb-886aab18fdf2",
   "metadata": {},
   "source": [
    "# Lets see how trained checkpoint performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0aa4113-cd8b-4f5f-b0c2-1f8f36fe4142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Local checkpoint path:\n",
      "checkpoints/6979cccf-c48e-4d3f-9543-3b0dee459f91 \n",
      "\n",
      "   Experiment ID |   Trial ID |   Steps Completed | Report Time              | Checkpoint UUID                      | Validation Metrics                       | Metadata\n",
      "-----------------+------------+-------------------+--------------------------+--------------------------------------+------------------------------------------+----------------------------------------\n",
      "              27 |         27 |                37 | 2023-03-23 21:08:15+0000 | 6979cccf-c48e-4d3f-9543-3b0dee459f91 | {                                        | {\n",
      "                 |            |                   |                          |                                      |     \"avgMetrics\": {                      |     \"determined_version\": \"0.20.0\",\n",
      "                 |            |                   |                          |                                      |         \"eval_loss\": 0.6380631327629089, |     \"format\": \"pickle\",\n",
      "                 |            |                   |                          |                                      |         \"perplexity\": 1.8969178199768066 |     \"framework\": \"torch-1.10.2+cu113\",\n",
      "                 |            |                   |                          |                                      |     },                                   |     \"steps_completed\": 37\n",
      "                 |            |                   |                          |                                      |     \"batchMetrics\": []                   | }\n",
      "                 |            |                   |                          |                                      | }                                        |\n"
     ]
    }
   ],
   "source": [
    "!det checkpoint download 6979cccf-c48e-4d3f-9543-3b0dee459f91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6de4502-05f5-440c-b099-bda4f03ef31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "ckpt = torch.load('./checkpoints/6979cccf-c48e-4d3f-9543-3b0dee459f91/state_dict.pth')\n",
    "# print(len(ckpt['models_state_dict']))\n",
    "loaded_model.load_state_dict(ckpt['models_state_dict'][0])\n",
    "latex_generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22a1e416-a94a-4132-b5fe-bfa2b34a6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX:\n"
     ]
    }
   ],
   "source": [
    "# text_sample = 'f of x equals integral from 0 to pi of x to the fourth power'\n",
    "text_sample = 'f of x is sum from 0 to x of x squared'\n",
    "\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(conversion_text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8694cc97-5c84-4349-a7ad-bb88db743be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7e22a5a-d3d0-4db2-8ea4-f467fb337eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f(x) = x^2 \\,dx^2 \\,dx^2 \\,\n"
     ]
    }
   ],
   "source": [
    "print(latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77d056a0-4fa2-49e8-abe1-2e344a5c47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"LCT\n",
    "English: f of x is sum from 0 to x of x squared\n",
    "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: f of x equals integral from 0 to pi of x to the fourth power\n",
    "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: f of x equals x squared\n",
    "LaTeX:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f75d8e2d-e1de-46fd-bdb6-06e618332710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx ###\n",
      "LCT\n",
      "English: f of x equals integral from 0 to pi of x to the fourth power\n",
      "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx ###\n",
      "LCT\n",
      "English: f of x equals x squared\n",
      "LaTeX: f(x) = \\int_{0}^{\\pi} x^2 \\,dx\n"
     ]
    }
   ],
   "source": [
    "print(latex_generator(\n",
    "    few_shot_prompt, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(few_shot_prompt)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f8e2a-7df8-4746-8e1d-77de58bdb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80fa480-c495-4f09-b0b9-aa300a77e884",
   "metadata": {},
   "source": [
    "# How does Multi-GPU and Multi-Node GPU improve finetuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219415c4-a72a-46f8-937d-8eda68fafead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77e299-2787-40bf-9c1e-b9711facbb31",
   "metadata": {},
   "source": [
    "# User Exercise 1: Finetune book to author of choice\n",
    "Lets form into groups, download text from online, and train our own chatbot!\n",
    "\n",
    "Steps to integrate custom dataset:\n",
    "* Go to project gutenburg and pick a book: (i.e. https://www.gutenberg.org/ebooks/1787 )\n",
    "* copy URL Plain Text UTF-8 .txt file and download using command: \n",
    "    - Example: `wget -O hamlet.txt https://www.gutenberg.org/cache/epub/1787/pg1787.txt`\n",
    "* move to shared directory: `cp hamlet.txt /run/determined/workdir/shared_fs/exercise/ -v`\n",
    "* Copy `run_det_ds_chatbot.sh` and rename: \n",
    "    - i.e. `cp gpt2-determined-finetune-poc/determined_files/const_ds_chatbot.yaml gpt2-determined-finetune-poc/determined_files/const_hamlet_chatbot.yaml `\n",
    "* NOTE: MAKE SURE THAT the file is a text file, and that there are no spaces in the name of the text file\n",
    "* Finally, change name that describes exp( name: gpt2_finetune_hamlet_chatbot) and change the dataset_name to name of text file: (i.e. hamlet)\n",
    "    - Do not include `.txt` in dataset name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b084b85-67e7-4607-9cef-33b4e55313ff",
   "metadata": {},
   "source": [
    "## Challenging User Exercise (Optional): Finetune book on code documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59ec8c-bacd-4956-ae66-551d38c829b3",
   "metadata": {},
   "source": [
    "## Here are a series of commands to scrape html pages and converge to one text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b382009d-a3d2-4118-9403-2fd8a98663fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install html2text\n",
    "!wget --mirror https://docs.zdetermined.ai/latest/ --accept html\n",
    "!find docs.determined.ai -type f -exec html2text -o - '{}' \\+ > determined.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed22d223-a965-4734-a1c5-48442c2dff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp determined.txt /run/determined/workdir/shared_fs/workshop_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5da1e6c4-efdc-49e9-b699-542a6cfb74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp determined_files/const_ds_chatbot.yaml determined_files/const_det_chatbot.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8950430-8356-42b7-9294-987f9d1b195e",
   "metadata": {},
   "source": [
    "edit the name and dataset_name in `const_det_chatbot.yaml `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a1fcf6-54e6-40b6-bc23-9673113abc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing files to send to master... 22.2KB and 15 files\n",
      "Created experiment 1075\n"
     ]
    }
   ],
   "source": [
    "!det experiment create determined_files/const_det_chatbot.yaml determined_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c8f247-e0c6-4f5c-be5a-73447d3890ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
