{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5c37a8-f4b6-45a3-ac61-cc8a0ac2962d",
   "metadata": {},
   "source": [
    "# Chicago DS Summit Workshop: Finetuning GPT models at Scale\n",
    " ----\n",
    "\n",
    "Note this Demo is based on https://github.com/pytorch/vision/tree/v0.11.3\n",
    "\n",
    "This notebook walks you through finetuning your own chatbot.\n",
    "We will learn waht are Generative Pretrained Transformers (GPT), how to finetune to your own domain, and finetune at Scale\n",
    "\n",
    "Overview of Tutorial:\n",
    "\n",
    "* What is a GPT model and Causal Modeling\n",
    "* Establish Baseline Chatbot to ask Data Science questions\n",
    "* Finetune GPT model on a Data Science book to better answer Data Science Tasks\n",
    "* Establish Baseline Chatbot to convert english to latex\n",
    "* Finetune GPT model on dataset to convert english to latex\n",
    "* (TODO) See benefit of pretraining on domain specific text (i.e. Calculus book) to improve English to Latex\n",
    "* (TODO) User exercises: Download Shakespeare book from Project Gutenburg, and finetune bot to speak like Shakespeare\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c271e3-0744-4c54-a4b9-dcd5dd9a4ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, GPT2LMHeadModel, pipeline, \\\n",
    "                         Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c557e7d-3d7c-4629-9b0a-efa9b1effbd5",
   "metadata": {},
   "source": [
    "# What is GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be00536-0b56-41f3-aeb6-4a64c04f9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a31e5b-8147-424f-8a5c-0c1367f47c6d",
   "metadata": {},
   "source": [
    "# Data Science Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b7537-1345-43d3-bef9-420a6b9266c5",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "`A hypothesis test is`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bdd68c9-e820-4872-8577-3a3ec23024d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 71.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # load up a GPT2 model\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', model=model, tokenizer='gpt2',\n",
    "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d3cbc58-8870-4bf3-a39a-d86b868cab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='A test statistic is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f0f56d2-6208-4a7e-ab34-c16b41e2698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "A test statistic is a measure of the success rate of a work from each country in getting this measure set up and using it.\n",
      "\n",
      "I would add that I don't know whether the American people consider this test \"common sense,\" or to what\n",
      "----------\n",
      "A test statistic is a fact. A standard deviation (standard deviation) of two values of the standard deviation of a statistic is a valid metric.\n",
      "\n",
      "So, if you think your normalization statistic is 4%, we can assume the mean deviation is\n",
      "----------\n",
      "A test statistic is in the neighborhood of $1.85.\n",
      "\n",
      "This suggests that, as of June of this year, all schools across California are failing. One reason for this is an ongoing lack of funding for primary education programs. So the\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('----------')\n",
    "for generated_sequence in pretrained_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86215-287d-47b8-8f16-d9942fd919cc",
   "metadata": {},
   "source": [
    "## Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d407a-0cd3-44b3-a909-b44f00d7ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/const_ds_chatbot.yaml \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb77c8-6010-43ff-9bb6-ef49ef61ff3e",
   "metadata": {},
   "source": [
    "# English 2 Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be3455-aec5-43de-ba08-186e3c1386ca",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5f14a9-2ce1-4453-94a4-fd9a5f3bb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that a non-finetuned model could not have done this\n",
    "MODEL='gpt2'\n",
    "non_finetuned_latex_generator = pipeline(\n",
    "    'text-generation', \n",
    "    model=GPT2LMHeadModel.from_pretrained(MODEL),  # not fine-tuned!\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3cb0b6-16f5-431a-985a-93c4a85bf94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f of x is\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'f of x is sum from 0 to x of x squared'\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(non_finetuned_latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e730dfb3-84ce-4160-8adb-bd165aba76f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"LCT\n",
    "English: f of x is sum from 0 to x of x squared\n",
    "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: f of x equals integral from 0 to pi of x to the fourth power\n",
    "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: x squared\n",
    "LaTeX:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb7d1341-94ee-4fb9-a85e-8ab6dfb8075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f of x is\n"
     ]
    }
   ],
   "source": [
    "print(non_finetuned_latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42ad3a-ad9f-4f4b-8f55-0530685757bd",
   "metadata": {},
   "source": [
    "## Dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3797380-ca2a-42fc-b718-3e20532faa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/english_to_latex.csv')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fa6f8-0b8f-436b-9dac-56ec209a0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8368f2a-4f52-4f55-840c-50edae606dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our singular prompt\n",
    "CONVERSION_PROMPT = 'LCT\\n'  # LaTeX conversion task\n",
    "\n",
    "CONVERSION_TOKEN = 'LaTeX:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccee9e-cc98-46ae-8f07-ac9e1a8e4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our \"training prompt\" that we want GPT2 to recognize and learn\n",
    "training_examples = f'{CONVERSION_PROMPT}English: ' + data['English'] + '\\n' + CONVERSION_TOKEN + ' ' + data['LaTeX'].astype(str)\n",
    "\n",
    "print(training_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c8483-2824-4449-b4fe-090369bfa462",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df = pd.DataFrame({'text': training_examples})\n",
    "\n",
    "task_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3a003-8be6-47e4-b924-8727725c90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_data = Dataset.from_pandas(task_df)  # turn a pandas DataFrame into a Dataset\n",
    "\n",
    "def preprocess(examples):  # tokenize our text but don't pad because our collator will pad for us dynamically\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "latex_data = latex_data.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb779568-71ab-42f0-bb62-b9edf4e0fc8b",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5f5f7-9d4a-4da5-9ad9-a31d5b216811",
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/const_eng_to_latex.yaml \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c70a3d-1660-4a5c-9beb-886aab18fdf2",
   "metadata": {},
   "source": [
    "# Lets see how trained checkpoint performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa4113-cd8b-4f5f-b0c2-1f8f36fe4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "!det checkpoint download 6979cccf-c48e-4d3f-9543-3b0dee459f91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f8e2a-7df8-4746-8e1d-77de58bdb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80fa480-c495-4f09-b0b9-aa300a77e884",
   "metadata": {},
   "source": [
    "# How does Multi-GPU and Multi-Node GPU improve finetuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219415c4-a72a-46f8-937d-8eda68fafead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
