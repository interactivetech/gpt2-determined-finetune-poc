{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5c37a8-f4b6-45a3-ac61-cc8a0ac2962d",
   "metadata": {},
   "source": [
    "# Chicago DS Summit Workshop: Finetuning GPT models at Scale\n",
    " ----\n",
    "\n",
    "Note this Demo is based on https://github.com/pytorch/vision/tree/v0.11.3\n",
    "\n",
    "This notebook walks you through finetuning your own chatbot.\n",
    "We will learn waht are Generative Pretrained Transformers (GPT), how to finetune to your own domain, and finetune at Scale\n",
    "\n",
    "Overview of Tutorial:\n",
    "\n",
    "* What is a GPT model and Causal Modeling\n",
    "* Establish Baseline Chatbot to ask Data Science questions\n",
    "* Finetune GPT model on a Data Science book to better answer Data Science Tasks\n",
    "* Establish Baseline Chatbot to convert english to latex\n",
    "* Finetune GPT model on dataset to convert english to latex\n",
    "* (TODO) See benefit of pretraining on domain specific text (i.e. Calculus book) to improve English to Latex\n",
    "* (TODO) User exercises: Download Shakespeare book from Project Gutenburg, and finetune bot to speak like Shakespeare\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97c271e3-0744-4c54-a4b9-dcd5dd9a4ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, GPT2LMHeadModel, pipeline, \\\n",
    "                         Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c557e7d-3d7c-4629-9b0a-efa9b1effbd5",
   "metadata": {},
   "source": [
    "# What is GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be00536-0b56-41f3-aeb6-4a64c04f9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a31e5b-8147-424f-8a5c-0c1367f47c6d",
   "metadata": {},
   "source": [
    "# Data Science Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b7537-1345-43d3-bef9-420a6b9266c5",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "`A hypothesis test is`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bdd68c9-e820-4872-8577-3a3ec23024d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 71.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # load up a GPT2 model\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', model=model, tokenizer='gpt2',\n",
    "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d3cbc58-8870-4bf3-a39a-d86b868cab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='A test statistic is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f0f56d2-6208-4a7e-ab34-c16b41e2698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "A test statistic is a measure of the success rate of a work from each country in getting this measure set up and using it.\n",
      "\n",
      "I would add that I don't know whether the American people consider this test \"common sense,\" or to what\n",
      "----------\n",
      "A test statistic is a fact. A standard deviation (standard deviation) of two values of the standard deviation of a statistic is a valid metric.\n",
      "\n",
      "So, if you think your normalization statistic is 4%, we can assume the mean deviation is\n",
      "----------\n",
      "A test statistic is in the neighborhood of $1.85.\n",
      "\n",
      "This suggests that, as of June of this year, all schools across California are failing. One reason for this is an ongoing lack of funding for primary education programs. So the\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('----------')\n",
    "for generated_sequence in pretrained_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86215-287d-47b8-8f16-d9942fd919cc",
   "metadata": {},
   "source": [
    "## Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "833d407a-0cd3-44b3-a909-b44f00d7ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Preparing files to send to master... 19.7KB and 14 files\n",
      "Created experiment 33\n"
     ]
    }
   ],
   "source": [
    "!det experiment create \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/const_ds_chatbot.yaml \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32982e-834b-404d-ba3e-e6adc3bff3e5",
   "metadata": {},
   "source": [
    "## See Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c54ab72-b109-4b9b-a877-ab6d4ca82435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Local checkpoint path:\n",
      "checkpoints/38443886-003c-4f28-97ae-108992665643 \n",
      "\n",
      "   Experiment ID |   Trial ID |   Steps Completed | Report Time              | Checkpoint UUID                      | Validation Metrics                       | Metadata\n",
      "-----------------+------------+-------------------+--------------------------+--------------------------------------+------------------------------------------+----------------------------------------\n",
      "              33 |         33 |               137 | 2023-03-24 03:00:52+0000 | 38443886-003c-4f28-97ae-108992665643 | {                                        | {\n",
      "                 |            |                   |                          |                                      |     \"avgMetrics\": {                      |     \"determined_version\": \"0.20.0\",\n",
      "                 |            |                   |                          |                                      |         \"eval_loss\": 3.0931387434200364, |     \"format\": \"pickle\",\n",
      "                 |            |                   |                          |                                      |         \"perplexity\": 22.546680450439453 |     \"framework\": \"torch-1.10.2+cu113\",\n",
      "                 |            |                   |                          |                                      |     },                                   |     \"steps_completed\": 137\n",
      "                 |            |                   |                          |                                      |     \"batchMetrics\": []                   | }\n",
      "                 |            |                   |                          |                                      | }                                        |\n"
     ]
    }
   ],
   "source": [
    "!det checkpoint download 38443886-003c-4f28-97ae-108992665643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e38e8fe0-9030-4d3a-bfdb-5b509450a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ./checkpoints/71dda373-9065-4536-81e0-4acceb7db318/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7e2582a-50fd-43f6-a1d3-efd7fcfced6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "ckpt = torch.load('./checkpoints/38443886-003c-4f28-97ae-108992665643/state_dict.pth')\n",
    "# print(len(ckpt['models_state_dict']))\n",
    "loaded_model.load_state_dict(ckpt['models_state_dict'][0])\n",
    "\n",
    "finetuned_generator = pipeline(\n",
    "    'text-generation', model=loaded_model, tokenizer=tokenizer,\n",
    "    config={'max_length': 200,  'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5dd448dc-f45a-4d29-8405-93c75721e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT='Sinan is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "624d94e5-11f1-4bd7-9518-c59eeac7c1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Sinan is not.\n",
      "We will see what happens. For ease of use and in order to\n",
      "make this book, let's go ahead and define a few parameters in order to\n",
      "optimize the data and the presentation in the\n",
      "future.\n",
      "----------\n",
      "Sinan is the best-known and best-known in mathematics. His most recent book, The Mathematics of Series Form, is well known for being considered one of the most important books on the subject of data science. His latest book on the topic\n",
      "----------\n",
      "Sinan is working on a very big project. He is trying to do away with a little more machine learning and focus on a much more\n",
      "complex set of datasets, the Bayesian data analysis.\n",
      "We may see a few different\n",
      "samples\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('----------')\n",
    "for generated_sequence in finetuned_generator(PROMPT, num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb77c8-6010-43ff-9bb6-ef49ef61ff3e",
   "metadata": {},
   "source": [
    "# English 2 Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be3455-aec5-43de-ba08-186e3c1386ca",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5f14a9-2ce1-4453-94a4-fd9a5f3bb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that a non-finetuned model could not have done this\n",
    "MODEL='gpt2'\n",
    "non_finetuned_latex_generator = pipeline(\n",
    "    'text-generation', \n",
    "    model=GPT2LMHeadModel.from_pretrained(MODEL),  # not fine-tuned!\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f3cb0b6-16f5-431a-985a-93c4a85bf94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX:\n",
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f of x is\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'f of x is sum from 0 to x of x squared'\n",
    "# text_sample = 'f of x equals x squared'\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(conversion_text_sample)\n",
    "print(non_finetuned_latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730dfb3-84ce-4160-8adb-bd165aba76f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb7d1341-94ee-4fb9-a85e-8ab6dfb8075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(non_finetuned_latex_generator(\n",
    "#     conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "#     max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    "# )[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42ad3a-ad9f-4f4b-8f55-0530685757bd",
   "metadata": {},
   "source": [
    "## Dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3797380-ca2a-42fc-b718-3e20532faa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/english_to_latex.csv')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fa6f8-0b8f-436b-9dac-56ec209a0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8368f2a-4f52-4f55-840c-50edae606dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our singular prompt\n",
    "CONVERSION_PROMPT = 'LCT\\n'  # LaTeX conversion task\n",
    "\n",
    "CONVERSION_TOKEN = 'LaTeX:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49ccee9e-cc98-46ae-8f07-ac9e1a8e4b92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This is our \"training prompt\" that we want GPT2 to recognize and learn\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONVERSION_PROMPT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mEnglish: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mdata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m CONVERSION_TOKEN \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLaTeX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_examples[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# This is our \"training prompt\" that we want GPT2 to recognize and learn\n",
    "training_examples = f'{CONVERSION_PROMPT}English: ' + data['English'] + '\\n' + CONVERSION_TOKEN + ' ' + data['LaTeX'].astype(str)\n",
    "\n",
    "print(training_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c8483-2824-4449-b4fe-090369bfa462",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df = pd.DataFrame({'text': training_examples})\n",
    "\n",
    "task_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3a003-8be6-47e4-b924-8727725c90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_data = Dataset.from_pandas(task_df)  # turn a pandas DataFrame into a Dataset\n",
    "\n",
    "def preprocess(examples):  # tokenize our text but don't pad because our collator will pad for us dynamically\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "latex_data = latex_data.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb779568-71ab-42f0-bb62-b9edf4e0fc8b",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5f5f7-9d4a-4da5-9ad9-a31d5b216811",
   "metadata": {},
   "outputs": [],
   "source": [
    "!det experiment create \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/const_eng_to_latex.yaml \\\n",
    "    /run/determined/workdir/gpt2-determined-finetune-poc/determined_files/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c70a3d-1660-4a5c-9beb-886aab18fdf2",
   "metadata": {},
   "source": [
    "# Lets see how trained checkpoint performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0aa4113-cd8b-4f5f-b0c2-1f8f36fe4142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Local checkpoint path:\n",
      "checkpoints/6979cccf-c48e-4d3f-9543-3b0dee459f91 \n",
      "\n",
      "   Experiment ID |   Trial ID |   Steps Completed | Report Time              | Checkpoint UUID                      | Validation Metrics                       | Metadata\n",
      "-----------------+------------+-------------------+--------------------------+--------------------------------------+------------------------------------------+----------------------------------------\n",
      "              27 |         27 |                37 | 2023-03-23 21:08:15+0000 | 6979cccf-c48e-4d3f-9543-3b0dee459f91 | {                                        | {\n",
      "                 |            |                   |                          |                                      |     \"avgMetrics\": {                      |     \"determined_version\": \"0.20.0\",\n",
      "                 |            |                   |                          |                                      |         \"eval_loss\": 0.6380631327629089, |     \"format\": \"pickle\",\n",
      "                 |            |                   |                          |                                      |         \"perplexity\": 1.8969178199768066 |     \"framework\": \"torch-1.10.2+cu113\",\n",
      "                 |            |                   |                          |                                      |     },                                   |     \"steps_completed\": 37\n",
      "                 |            |                   |                          |                                      |     \"batchMetrics\": []                   | }\n",
      "                 |            |                   |                          |                                      | }                                        |\n"
     ]
    }
   ],
   "source": [
    "!det checkpoint download 6979cccf-c48e-4d3f-9543-3b0dee459f91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6de4502-05f5-440c-b099-bda4f03ef31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "ckpt = torch.load('./checkpoints/6979cccf-c48e-4d3f-9543-3b0dee459f91/state_dict.pth')\n",
    "# print(len(ckpt['models_state_dict']))\n",
    "loaded_model.load_state_dict(ckpt['models_state_dict'][0])\n",
    "latex_generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22a1e416-a94a-4132-b5fe-bfa2b34a6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX:\n"
     ]
    }
   ],
   "source": [
    "# text_sample = 'f of x equals integral from 0 to pi of x to the fourth power'\n",
    "text_sample = 'f of x is sum from 0 to x of x squared'\n",
    "\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(conversion_text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8694cc97-5c84-4349-a7ad-bb88db743be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b7e22a5a-d3d0-4db2-8ea4-f467fb337eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCT\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f(x) = x^2 \\,dx^2 \\,dx^2 \\,\n"
     ]
    }
   ],
   "source": [
    "print(latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f8e2a-7df8-4746-8e1d-77de58bdb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80fa480-c495-4f09-b0b9-aa300a77e884",
   "metadata": {},
   "source": [
    "# How does Multi-GPU and Multi-Node GPU improve finetuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219415c4-a72a-46f8-937d-8eda68fafead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
